{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOvbfy4u4219wAmY5gPB7+m",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MSaiTeja01/BDS_Assignment2/blob/main/BDS_Assignment2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "gAMqE_dvRXqO"
      },
      "outputs": [],
      "source": [
        "# 1. Setup Apache Spark in Google Colab\n",
        "\n",
        "\n",
        "# Install Java 8\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "\n",
        "# Download and extract Spark (here we use Spark 3.1.2 with Hadoop 3.2)\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.1.2/spark-3.1.2-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.1.2-bin-hadoop3.2.tgz\n",
        "\n",
        "# Set environment variables for Java and Spark\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.2-bin-hadoop3.2\"\n",
        "\n",
        "# Install findspark to make Spark available in the Python environment\n",
        "!pip install -q findspark\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "# Create a SparkSession\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").appName(\"WebLogAnalysis\").getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Load the Data\n",
        "\n",
        "# Load each compressed log file as a DataFrame\n",
        "df_jul = spark.read.text(\"/content/Access_log_Aug1995.gz\")\n",
        "df_aug = spark.read.text(\"/content/Access_log_Jul1995.gz\")\n",
        "\n",
        "# Combine both months into one DataFrame\n",
        "df_all = df_jul.union(df_aug)"
      ],
      "metadata": {
        "id": "QA3WYeLZSHEz"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Data Wrangling\n",
        "\n",
        "# a. Parse the Log Files\n",
        "\n",
        "from pyspark.sql.functions import regexp_extract, col\n",
        "\n",
        "# Regular expression pattern based on the Common Log Format\n",
        "regex_pattern = r'(\\S+) (\\S+) (\\S+) \\[(.*?)\\] \"(.*?)\" (\\d{3}) (\\S+)'\n",
        "\n",
        "logs_df = df_all.select(\n",
        "    regexp_extract('value', regex_pattern, 1).alias('remotehost'),\n",
        "    regexp_extract('value', regex_pattern, 2).alias('rfc931'),\n",
        "    regexp_extract('value', regex_pattern, 3).alias('authuser'),\n",
        "    regexp_extract('value', regex_pattern, 4).alias('date'),\n",
        "    regexp_extract('value', regex_pattern, 5).alias('request'),\n",
        "    regexp_extract('value', regex_pattern, 6).alias('status'),\n",
        "    regexp_extract('value', regex_pattern, 7).alias('bytes')\n",
        ")\n"
      ],
      "metadata": {
        "id": "dHSK0JT4SPU4"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# b. Handle Missing or Malformed Data\n",
        "from pyspark.sql.functions import when\n",
        "\n",
        "logs_df = logs_df.withColumn(\"rfc931\", when(col(\"rfc931\")==\"-\", None).otherwise(col(\"rfc931\")))\n",
        "logs_df = logs_df.withColumn(\"authuser\", when(col(\"authuser\")==\"-\", None).otherwise(col(\"authuser\")))\n",
        "logs_df = logs_df.withColumn(\"bytes\", when(col(\"bytes\")==\"-\", None).otherwise(col(\"bytes\")))\n",
        "logs_df = logs_df.withColumn(\"status\", col(\"status\").cast(\"int\"))\n",
        "logs_df = logs_df.withColumn(\"bytes\", when(col(\"bytes\").isNotNull(), col(\"bytes\").cast(\"int\")).otherwise(None))\n"
      ],
      "metadata": {
        "id": "WIHiXGHlTnnQ"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# c. Convert and Format Date\n",
        "from pyspark.sql.functions import to_timestamp, date_format\n",
        "\n",
        "# The date field in the log is in the format: dd/MMM/yyyy:HH:mm:ss (ignoring zone)\n",
        "# In Spark 3+, the timestamp parser is stricter. We'll handle timezone separately.\n",
        "logs_df = logs_df.withColumn(\"timestamp\", to_timestamp(col(\"date\").substr(1, 20), \"dd/MMM/yyyy:HH:mm:ss\")) # Using substring to remove brackets and timezone\n",
        "logs_df = logs_df.withColumn(\"date_formatted\", date_format(col(\"timestamp\"), \"dd-MMM-yyyy\"))"
      ],
      "metadata": {
        "id": "eYB_HSNITqoF"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# d. Extract the Endpoint from the Request\n",
        "from pyspark.sql.functions import split\n",
        "\n",
        "# The request field is typically: \"GET /path/resource HTTP/1.0\"\n",
        "logs_df = logs_df.withColumn(\"endpoint\", split(col(\"request\"), \" \").getItem(1))\n"
      ],
      "metadata": {
        "id": "n49YZpzQTtdu"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Analytics\n",
        "\n",
        "# i. Total Log Records\n",
        "total_logs = logs_df.count()\n",
        "print(\"Total Log Records:\", total_logs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D8BxpjOwTy1q",
        "outputId": "cd10a409-a6eb-4567-c90e-1a3c008a5449"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Log Records: 3461613\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ii. Count of Unique Hosts\n",
        "unique_hosts = logs_df.select(\"remotehost\").distinct().count()\n",
        "print(\"Unique Hosts:\", unique_hosts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "litVoLpXT3Qo",
        "outputId": "400e678b-f238-4775-caa4-af8aa9c6bdef"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique Hosts: 137979\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# iii. Date-wise Unique Host Counts\n",
        "from pyspark.sql.functions import countDistinct\n",
        "\n",
        "date_unique_hosts = logs_df.groupBy(\"date_formatted\") \\\n",
        "                           .agg(countDistinct(\"remotehost\").alias(\"unique_hosts\")) \\\n",
        "                           .orderBy(\"date_formatted\")\n",
        "date_unique_hosts.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s7Au3W80UhbJ",
        "outputId": "8b309801-e530-4e43-f1e9-b8cf68b8f748"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+------------+\n",
            "|date_formatted|unique_hosts|\n",
            "+--------------+------------+\n",
            "|null          |1           |\n",
            "|01-Aug-1995   |2582        |\n",
            "|01-Jul-1995   |5192        |\n",
            "|02-Jul-1995   |4859        |\n",
            "|03-Aug-1995   |3222        |\n",
            "|03-Jul-1995   |7336        |\n",
            "|04-Aug-1995   |4191        |\n",
            "|04-Jul-1995   |5524        |\n",
            "|05-Aug-1995   |2502        |\n",
            "|05-Jul-1995   |7383        |\n",
            "|06-Aug-1995   |2538        |\n",
            "|06-Jul-1995   |7820        |\n",
            "|07-Aug-1995   |4108        |\n",
            "|07-Jul-1995   |6474        |\n",
            "|08-Aug-1995   |4406        |\n",
            "|08-Jul-1995   |2898        |\n",
            "|09-Aug-1995   |4317        |\n",
            "|09-Jul-1995   |2554        |\n",
            "|10-Aug-1995   |4523        |\n",
            "|10-Jul-1995   |4464        |\n",
            "+--------------+------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# iv. Average Requests per Host per Day\n",
        "from pyspark.sql.functions import count\n",
        "\n",
        "# Total requests per day\n",
        "daily_requests = logs_df.groupBy(\"date_formatted\") \\\n",
        "                        .agg(count(\"request\").alias(\"total_requests\"))\n",
        "\n",
        "# Unique hosts per day (already calculated)\n",
        "daily_unique_hosts = logs_df.groupBy(\"date_formatted\") \\\n",
        "                            .agg(countDistinct(\"remotehost\").alias(\"unique_hosts\"))\n",
        "\n",
        "# Join and calculate average\n",
        "daily_avg = daily_requests.join(daily_unique_hosts, \"date_formatted\") \\\n",
        "                          .withColumn(\"avg_requests_per_host\", col(\"total_requests\") / col(\"unique_hosts\")) \\\n",
        "                          .orderBy(\"date_formatted\")\n",
        "daily_avg.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bt9NsKQCUnKV",
        "outputId": "be709202-3753-4264-b78b-a641415b4a0d"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+--------------+------------+---------------------+\n",
            "|date_formatted|total_requests|unique_hosts|avg_requests_per_host|\n",
            "+--------------+--------------+------------+---------------------+\n",
            "|01-Aug-1995   |33996         |2582        |13.166537567776917   |\n",
            "|01-Jul-1995   |64714         |5192        |12.464175654853621   |\n",
            "|02-Jul-1995   |60265         |4859        |12.40275776908829    |\n",
            "|03-Aug-1995   |41388         |3222        |12.845437616387336   |\n",
            "|03-Jul-1995   |89584         |7336        |12.211559432933479   |\n",
            "|04-Aug-1995   |59557         |4191        |14.210689572894298   |\n",
            "|04-Jul-1995   |70452         |5524        |12.753801593048516   |\n",
            "|05-Aug-1995   |31893         |2502        |12.747002398081534   |\n",
            "|05-Jul-1995   |94575         |7383        |12.809833401056482   |\n",
            "|06-Aug-1995   |32420         |2538        |12.77383766745469    |\n",
            "|06-Jul-1995   |100960        |7820        |12.910485933503836   |\n",
            "|07-Aug-1995   |57362         |4108        |13.9634858812074     |\n",
            "|07-Jul-1995   |87233         |6474        |13.474358974358974   |\n",
            "|08-Aug-1995   |60157         |4406        |13.653427144802542   |\n",
            "|08-Jul-1995   |38867         |2898        |13.411663216011043   |\n",
            "|09-Aug-1995   |60458         |4317        |14.00463284688441    |\n",
            "|09-Jul-1995   |35272         |2554        |13.810493343774471   |\n",
            "|10-Aug-1995   |61248         |4523        |13.541454786646032   |\n",
            "|10-Jul-1995   |72860         |4464        |16.32168458781362    |\n",
            "|11-Aug-1995   |61246         |4346        |14.092498849516797   |\n",
            "+--------------+--------------+------------+---------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# v. Number of 404 Response Codes\n",
        "errors_404 = logs_df.filter(col(\"status\") == 404).count()\n",
        "print(\"Total 404 Errors:\", errors_404)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XWvHfs8AVHBY",
        "outputId": "a885e943-b156-4dba-daae-44576448630e"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total 404 Errors: 20901\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# vi. Top 15 Endpoints with 404 Responses\n",
        "from pyspark.sql.functions import desc\n",
        "\n",
        "endpoints_404 = logs_df.filter(col(\"status\") == 404) \\\n",
        "                       .groupBy(\"endpoint\") \\\n",
        "                       .agg(count(\"endpoint\").alias(\"error_count\")) \\\n",
        "                       .orderBy(desc(\"error_count\")) \\\n",
        "                       .limit(15)\n",
        "endpoints_404.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RU1IPnJDVocr",
        "outputId": "04afdf90-a461-4445-8de2-224a0a249f92"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------------------------------------------------------+-----------+\n",
            "|endpoint                                                         |error_count|\n",
            "+-----------------------------------------------------------------+-----------+\n",
            "|/pub/winvn/readme.txt                                            |2004       |\n",
            "|/pub/winvn/release.txt                                           |1732       |\n",
            "|/shuttle/missions/STS-69/mission-STS-69.html                     |683        |\n",
            "|/shuttle/missions/sts-68/ksc-upclose.gif                         |428        |\n",
            "|/history/apollo/a-001/a-001-patch-small.gif                      |384        |\n",
            "|/history/apollo/sa-1/sa-1-patch-small.gif                        |383        |\n",
            "|/://spacelink.msfc.nasa.gov                                      |381        |\n",
            "|/images/crawlerway-logo.gif                                      |374        |\n",
            "|/elv/DELTA/uncons.htm                                            |372        |\n",
            "|/history/apollo/pad-abort-test-1/pad-abort-test-1-patch-small.gif|359        |\n",
            "|/images/nasa-logo.gif                                            |319        |\n",
            "|/shuttle/resources/orbiters/atlantis.gif                         |314        |\n",
            "|/history/apollo/apollo-13.html                                   |304        |\n",
            "|/shuttle/resources/orbiters/discovery.gif                        |263        |\n",
            "|/shuttle/missions/sts-71/images/KSC-95EC-0916.txt                |190        |\n",
            "+-----------------------------------------------------------------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# vii. Top 15 Hosts with 404 Responses\n",
        "hosts_404 = logs_df.filter(col(\"status\") == 404) \\\n",
        "                   .groupBy(\"remotehost\") \\\n",
        "                   .agg(count(\"remotehost\").alias(\"error_count\")) \\\n",
        "                   .orderBy(desc(\"error_count\")) \\\n",
        "                   .limit(15)\n",
        "hosts_404.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-DPSxa8cVuCc",
        "outputId": "3f4468b2-9729-4edb-bf6a-d18d0d03f69b"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------------------+-----------+\n",
            "|remotehost                 |error_count|\n",
            "+---------------------------+-----------+\n",
            "|hoohoo.ncsa.uiuc.edu       |251        |\n",
            "|piweba3y.prodigy.com       |157        |\n",
            "|jbiagioni.npt.nuwc.navy.mil|132        |\n",
            "|piweba1y.prodigy.com       |114        |\n",
            "|www-d4.proxy.aol.com       |91         |\n",
            "|piweba4y.prodigy.com       |86         |\n",
            "|scooter.pa-x.dec.com       |69         |\n",
            "|www-d1.proxy.aol.com       |64         |\n",
            "|phaelon.ksc.nasa.gov       |64         |\n",
            "|dialip-217.den.mmc.com     |62         |\n",
            "|www-b4.proxy.aol.com       |62         |\n",
            "|www-b3.proxy.aol.com       |61         |\n",
            "|www-a2.proxy.aol.com       |60         |\n",
            "|titan02f                   |59         |\n",
            "|piweba2y.prodigy.com       |59         |\n",
            "+---------------------------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OcMs8wpGV-3i"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}